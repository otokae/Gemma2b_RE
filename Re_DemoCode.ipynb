{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ed1c5-0184-4ab5-ac4b-05c5848bd3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b55ae-71b6-4d98-8c1c-e4ce63f5e1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#GW: I added this pip install\n",
    "!pip install SpeechRecognition\n",
    "!pip install google-cloud-texttospeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38194b-fbf5-4632-bfd4-c5d1f69dfca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Gradio\n",
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9f9701-b33a-47ee-9893-e718cc88e350",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f1e5308c3e44fb8a89c0ab46be3960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1 RUN THIS FIRST FOR THE DEMO WITHOUT MY_MODEL.KERAS USING GEMMA2BINSTRUCT INSTEAD.\n",
    "\n",
    "#start off running this for the gemma 2b instruct model\n",
    "# import keras so it is defined for later use\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Load the saved model in .keras format\n",
    "# Load the saved model in .keras format\n",
    "#GW: Note that I don't have your model, so I'm loading a default one.\n",
    "#GW: Need to get permissions via KaggleHub thus the login step.\n",
    "#GW: gemma_lm = keras.models.load_model('my_model.keras')\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8ffe7-c2bb-4089-996d-d54e1838a2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173260ca-a00c-43ba-862d-6d9b70b6b0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2 FOR THE DEMO WITHOUT  MY_MODEL.KERAS \n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5d89e-9848-45d9-af11-6597bca69ee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 FOR THE DEMO run this first to load the Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64ef57-8450-48df-b9ab-638406722ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1 run this first to load the finetuned model\n",
    "#start off running this\n",
    "# import keras so it is defined for later use\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Load the saved model in .keras format\n",
    "gemma_lm = keras.models.load_model('my_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95315a6-a0b7-42e4-a615-4d9e6e0f0054",
   "metadata": {},
   "source": [
    "## 2 FOR THE DEMO run this after you load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b316c-d3d2-499f-a3fc-93f901ccf664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2 FOR THE DEMO run this after you load the model\n",
    "import datetime\n",
    "import gradio as gr\n",
    "import tensorflow as tfG\n",
    "import keras\n",
    "import speech_recognition as sr\n",
    "from google.cloud import texttospeech\n",
    "\n",
    "#GW: I moved the imports to the cell above\n",
    "\n",
    "# Initialize the Text-to-Speech client\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# Initialize the Speech Recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Function to process Speech-to-Text\n",
    "def speech_to_text(audio):\n",
    "    try:\n",
    "        print(\"sr.AudioFile start: \", datetime.datetime.now(), \"audio=\", type(audio), audio)\n",
    "        with sr.AudioFile(audio) as source:\n",
    "            print(\"sr.AudioFile start2: \", datetime.datetime.now(), \"source=\", type(source))\n",
    "            audio_data = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_data)\n",
    "            print(\"sr.AudioFile end: \", datetime.datetime.now())\n",
    "            return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Sorry, I couldn't understand the audio. Please try again.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Request error from Google STT service; {e}\"\n",
    "\n",
    "# Function to generate response from the chatbot model\n",
    "def generate_response(model: keras.Model, question: str, max_length: int = 250) -> str:\n",
    "    prompt_template = \"\"\" Role: You will use layman english to explain yourself. You will explain everything in simple terms using layman english.\n",
    "    You are a helpful real estate assistant providing support to legal matters in Ontario, \n",
    "    Canada Region; always answer the questions based on the following instructions; given the question, \n",
    "    generate an answer in 5 to 6 sentences always. For every statement in the answer, provide supporting \n",
    "    legal document or law segments from Ontario, Canada region.\n",
    "    Question:\\n{question}\\n\\nAnswer:\\n\"\"\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=question)\n",
    "    print(\"generate_response start: \", datetime.datetime.now(), \"prompt-->\", prompt,\"<--\")\n",
    "    \n",
    "    try:\n",
    "        generated_response = model.generate(prompt, max_length=max_length)\n",
    "        generated_text = generated_response.decode('utf-8') if isinstance(generated_response, bytes) else generated_response\n",
    "        answer_text = generated_text[len(prompt):].strip()\n",
    "        print(\"generate_response generated: \", datetime.datetime.now(), \"answer text-->\", answer_text, \"<--\")\n",
    "        return answer_text\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Error generating answer: {e}\"\n",
    "\n",
    "# Function to synthesize text using Google Text-to-Speech\n",
    "def synthesize_text(text):\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        name=\"en-US-Standard-C\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    \n",
    "    print(\"client synthesize_speech start: \", datetime.datetime.now())\n",
    "    response = client.synthesize_speech(\n",
    "        request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n",
    "    )\n",
    "    print(\"client synthesize_speech finished: \", datetime.datetime.now())\n",
    "    audio_file_path = \"output.mp3\"\n",
    "    with open(audio_file_path, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    return audio_file_path\n",
    "\n",
    "# Combined function to handle chatbot response and TTS\n",
    "def chatbot_with_tts_and_stt(audio):\n",
    "    print(\"chatbot start: \", datetime.datetime.now())\n",
    "    # Convert speech to text\n",
    "    user_input = speech_to_text(audio)\n",
    "    if not user_input.strip():\n",
    "        return \"Chatbot: I'm here to help! Please try speaking again.\", None\n",
    "    \n",
    "    # Confirm the input text\n",
    "    confirmation_message = f\"Recognized text: '{user_input}'. Is this correct? If so, proceed. If not, try again.\"\n",
    "    # Optionally, you can add a confirmation mechanism here\n",
    "    \n",
    "    # Generate chatbot response\n",
    "    answer = generate_response(gemma_lm, user_input, max_length=250)\n",
    "    \n",
    "    # Synthesize the generated text into speech\n",
    "    audio_file_path = synthesize_text(answer)\n",
    "    \n",
    "    print(\"chatbot finish: \", datetime.datetime.now())\n",
    "    return answer, audio_file_path\n",
    "\n",
    "# Create Gradio interface THIS LOOKS GOOD NEED REMOVE WATERMARK CHARACTER\n",
    "interface = gr.Interface(\n",
    "    fn=chatbot_with_tts_and_stt,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
    "    outputs=[gr.Textbox(), gr.Audio(autoplay=True)],\n",
    "    title=\"\"\"\n",
    "    <div style=\"display: flex; flex-direction: column; align-items: center; text-align: center; margin-bottom: 10px;\">\n",
    "        <img src=\"http://getmuggle.com/wp-content/uploads/2024/11/Logo-shrunk.png\" style=\"height: 180px; margin-bottom: -10px;\">\n",
    "        <h1 style=\"margin: 0; font-size: 24px;\">Real Estate Mate</h1>\n",
    "    </div>\n",
    "    \"\"\",\n",
    "    description=\"Speak your question and get a spoken answer.\",\n",
    "    # theme='Zarkel/IBM_Carbon_Theme'\n",
    "    theme='NoCrypt/miku'\n",
    "    # theme='shivi/calm_seafoam'\n",
    "    # theme='gstaff/xkcd'\n",
    "    # theme='NoCrypt/miku'\n",
    "    # theme='Base'\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo = interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444cdcb4-0638-4a82-a860-58f5f1f8848f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1c4cc-6998-4d14-a3df-9aa28087ed2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584f8da-4847-483f-95cb-98d75cdb9053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c94ae5-c2af-4994-b06e-97d2e076a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to change the model run this to clear the memory\n",
    "import gc\n",
    "\n",
    "del gemma_lm  # Delete the model\n",
    "gc.collect()  # Run garbage collection to free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307bd71-52e1-419a-9179-d0b3ce375a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
