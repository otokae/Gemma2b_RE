{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP THE ENVIORNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eeBtYqJsZPG",
    "outputId": "d0645149-4fe7-4304-81dd-cb18354cd7c9",
    "papermill": {
     "duration": 29.215629,
     "end_time": "2024-02-21T09:38:03.131031",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.915402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eeBtYqJsZPG",
    "outputId": "d0645149-4fe7-4304-81dd-cb18354cd7c9",
    "papermill": {
     "duration": 29.215629,
     "end_time": "2024-02-21T09:38:03.131031",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.915402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZs8XXqUKRmi",
    "papermill": {
     "duration": 0.008992,
     "end_time": "2024-02-21T09:38:03.193546",
     "exception": false,
     "start_time": "2024-02-21T09:38:03.184554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPORT PACKAGES\n",
    "\n",
    "Import Keras and KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYHyPUA9hKTf",
    "papermill": {
     "duration": 13.723138,
     "end_time": "2024-02-21T09:38:16.925885",
     "exception": false,
     "start_time": "2024-02-21T09:38:03.202747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET KAGGLE CREDENTIALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installed Kaggle Hub to login\n",
    "\n",
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LOAD TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "file_path = 'final_laws.jsonl'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        \n",
    "        # Format the example with Question and Answer\n",
    "        template = \"Question:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "        \n",
    "        # Add the formatted string to data\n",
    "        data.append(template.format(**features))\n",
    "\n",
    "# use 996 training examples, to keep it fast in the beginning used 100 examples\n",
    "data = data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#show the data head\n",
    "\n",
    "# Print the first few examples (head) with entry numbers\n",
    "for idx, item in enumerate(data[:5], start=1):  # Adjust the number as needed\n",
    "    print(f\"Entry {idx}:{item}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_L6A5J-1QgC",
    "papermill": {
     "duration": 0.010947,
     "end_time": "2024-02-21T09:39:10.55264",
     "exception": false,
     "start_time": "2024-02-21T09:39:10.541693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference before fine tuning\n",
    "\n",
    "In this section, you will query the model with various prompts to see how it responds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVLXadptyo34",
    "papermill": {
     "duration": 0.010445,
     "end_time": "2024-02-21T09:39:10.573368",
     "exception": false,
     "start_time": "2024-02-21T09:39:10.562923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Buy a Home Prompt\n",
    "\n",
    "### Query the model for suggestions on how to buy a home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RCE3fdGhDE5",
    "papermill": {
     "duration": 0.008892,
     "end_time": "2024-02-21T09:38:17.316544",
     "exception": false,
     "start_time": "2024-02-21T09:38:17.307652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras_nlp\n",
    "\n",
    "# Initialize the gemma_lm model\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n",
    "\n",
    "\n",
    "# Define the template for the prompt\n",
    "template = \"Question:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "\n",
    "# Format the prompt with a question and leave the answer blank for model generation\n",
    "prompt = template.format(\n",
    "    Question=\"How can I buy a new house?\",\n",
    "    Answer=\"\",  # Leaving the answer blank for the model to generate it\n",
    ")\n",
    "\n",
    "# Generate the response using gemma_lm model\n",
    "print(gemma_lm.generate(prompt, max_length=256))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AePQUIs2h-Ks",
    "papermill": {
     "duration": 0.010616,
     "end_time": "2024-02-21T09:39:27.537776",
     "exception": false,
     "start_time": "2024-02-21T09:39:27.52716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model just responds with a basic advice on how to buy a home. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ74Zz_S0iVv",
    "papermill": {
     "duration": 0.010188,
     "end_time": "2024-02-21T09:39:27.558437",
     "exception": false,
     "start_time": "2024-02-21T09:39:27.548249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ELI5 How to buy a Home Prompt\n",
    "\n",
    "Prompt the model to explain buying a home  in terms simple enough that a child can understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lorJMbsusgoo",
    "outputId": "bff2c70f-b0f8-4402-c005-325861515c9a",
    "papermill": {
     "duration": 5.742136,
     "end_time": "2024-02-21T09:39:33.310999",
     "exception": false,
     "start_time": "2024-02-21T09:39:27.568863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    Question=\"Explain the process of buying a home in a way that a 5 year old child could understand.\",\n",
    "    Answer=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBQieduRizZf",
    "papermill": {
     "duration": 0.010874,
     "end_time": "2024-02-21T09:39:33.333088",
     "exception": false,
     "start_time": "2024-02-21T09:39:33.322214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The responses contains words that might not be easy to understand for a child such as budget, contract, reject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FINE-TUNING CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtj1MWG2HBPv",
    "papermill": {
     "duration": 0.094695,
     "end_time": "2024-02-21T09:52:24.685134",
     "exception": false,
     "start_time": "2024-02-21T09:52:24.590439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## For demonstration purposes, the model was fine-tuned on a small subset of the dataset for just one epoch and with a low LoRA rank value. To get better responses from the fine-tuned model, these values were experimented with:\n",
    "\n",
    "1. Increasing the size of the fine-tuning dataset\n",
    "2. Training for more steps (epochs)\n",
    "3. Setting a higher LoRA rank\n",
    "4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCucu6oHz53G",
    "outputId": "0d8c80d7-0ab5-4fd3-e219-b2df4464084c",
    "papermill": {
     "duration": 0.511035,
     "end_time": "2024-02-21T09:39:33.876166",
     "exception": false,
     "start_time": "2024-02-21T09:39:33.365131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## LoRA Fine-tuning\n",
    "\n",
    "## To get better responses from the model, fine-tune the model with Low Rank Adaptation (LoRA) using the real estate law for ontario canada dataset\n",
    "\n",
    "## Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQQ47kcdpbZ9",
    "papermill": {
     "duration": 0.011797,
     "end_time": "2024-02-21T09:39:33.903795",
     "exception": false,
     "start_time": "2024-02-21T09:39:33.891998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.5 billion to 1.3 million)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will continue training from this point\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "file_path = './kaggle/input/mydata-final-laws/mydata/final_laws.jsonl'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        \n",
    "        # Format the example with Question and Answer\n",
    "        template = \"Question:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "        \n",
    "        # Add the formatted string to data\n",
    "        data.append(template.format(**features))\n",
    "\n",
    "# Only use 100 training examples, to keep it fast\n",
    "data = data[:996]\n",
    "\n",
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 512\n",
    "\n",
    "# Use AdamW (a common optimizer for transformer models) with a slightly higher learning rate\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,  # Increased learning rate for faster convergence\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "# Compile the model\n",
    "gemma_lm.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Define EarlyStopping callback to monitor 'sparse_categorical_accuracy'\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='sparse_categorical_accuracy',  # Monitor accuracy\n",
    "    patience=3,  # Stop after 3 epochs of no improvement\n",
    "    mode='max',  # Stop when accuracy stops increasing\n",
    "    restore_best_weights=True  # Restore weights from the epoch with the best performance\n",
    ")\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate if accuracy stops improving\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='sparse_categorical_accuracy',\n",
    "    factor=0.5,  # Reduce learning rate by half when accuracy plateaus\n",
    "    patience=2,  # Number of epochs to wait before reducing learning rate\n",
    "    min_lr=1e-7  # Set a minimum learning rate to prevent over-reduction\n",
    ")\n",
    "\n",
    "# Fit the model with early stopping and learning rate scheduler\n",
    "gemma_lm.fit(\n",
    "    data, \n",
    "    epochs=20, \n",
    "    batch_size=2,  # Increased batch size for better gradient updates\n",
    "    callbacks=[early_stopping, lr_scheduler]  # Added learning rate scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model after it has been fine-tuned for resume later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Save the Keras model in .keras format\n",
    "gemma_lm.save('./kaggle/input/my_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD AND LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"joelbest/gemma2b_reo/keras/default\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 19:12:30.537383: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 19:12:30.557531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733080350.580162   15742 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733080350.586704   15742 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 19:12:30.608663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1733080354.621826   15742 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
      "  instance.compile_from_config(compile_config)\n"
     ]
    }
   ],
   "source": [
    "# run this first to load the finetuned model# import keras so it is defined for later use\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Load the saved model in .keras format\n",
    "gemma_lm = keras.models.load_model('/home/jupyter/.cache/kagglehub/models/joelbest/gemma2b_reo/keras/default/1/my_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output to show the model is sucessfully loaded\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "import keras\n",
    "import keras_nlp\n",
    "# Load the saved model with the built-in tokenizer and language model\n",
    "gemma_lm = keras.models.load_model('my_model.keras')\n",
    "\n",
    "# Example input texts for inference\n",
    "texts = [\n",
    "    \n",
    "    \"Do I need a lawyer to handle the closing process when purchasing a property in Ontario?\",\n",
    "    \"What are the legal implications of a co-ownership agreement when buying a property with someone else?\",\n",
    "    \"Can I negotiate a lower real estate commission rate with my agent?\",\n",
    "    \"What should I know about the rights and responsibilities for maintaining a shared fence between properties?\"\n",
    "]\n",
    "\n",
    "# Set a smaller batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Perform inference in batches\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "       \n",
    "  # Use the model to generate answers directly\n",
    "    answers = gemma_lm.generate(batch_texts)\n",
    "    \n",
    "    # Print the input questions and the generated answers\n",
    "    for text, answer in zip(batch_texts, answers):\n",
    "        print(f\"Question: {text}\")\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf# List available GPUs and their details\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        details = tf.config.experimental.get_memory_info(gpu.name)\n",
    "        print(f\"GPU: {gpu.name}\")\n",
    "        print(f\"  Current Memory Usage: {details['current']} bytes\")\n",
    "        print(f\"  Peak Memory Usage: {details['peak']} bytes\")\n",
    "else:\n",
    "    print(\"No GPU devices found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE THE MODEL AGAIN #\n",
    "\n",
    "gemma_lm.save('./kaggle/input/my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the history of epochs trained so far \n",
    "# Assuming you have the history object from model.fit()\n",
    "history = model.fit(...)\n",
    "\n",
    "# To get the number of epochs that were run\n",
    "num_epochs = len(history.epoch)\n",
    "print(f\"Number of epochs run: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yd-1cNw1dTn",
    "papermill": {
     "duration": 0.092253,
     "end_time": "2024-02-21T09:52:07.553072",
     "exception": false,
     "start_time": "2024-02-21T09:52:07.460819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## INFERENCE AFTER FINE TUNING\n",
    "After fine-tuning, responses follow the instruction provided in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H55JYJ1a1Kos",
    "papermill": {
     "duration": 0.092097,
     "end_time": "2024-02-21T09:52:07.73732",
     "exception": false,
     "start_time": "2024-02-21T09:52:07.645223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Buy a home Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7cDJHy8WfCB",
    "outputId": "5f67d5b9-826e-4d28-9be1-e95d295010b0",
    "papermill": {
     "duration": 14.645082,
     "end_time": "2024-02-21T09:52:22.473375",
     "exception": false,
     "start_time": "2024-02-21T09:52:07.828293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    Question=\"How can I buy a new house?\",\n",
    "    Answer=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXP6gg2mjs6u",
    "papermill": {
     "duration": 0.104088,
     "end_time": "2024-02-21T09:52:22.681301",
     "exception": false,
     "start_time": "2024-02-21T09:52:22.577213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model now explains how to buy a home in Ontario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7nVd8Mi1Yta",
    "papermill": {
     "duration": 0.093425,
     "end_time": "2024-02-21T09:52:22.874292",
     "exception": false,
     "start_time": "2024-02-21T09:52:22.780867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ELI5 How to Buy a home Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-2sYl2jqwl7",
    "outputId": "1d1f174b-508c-434b-8ae2-6ea517d49a37",
    "papermill": {
     "duration": 1.343147,
     "end_time": "2024-02-21T09:52:24.310022",
     "exception": false,
     "start_time": "2024-02-21T09:52:22.966875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    Question=\"Explain the process of buying a home in a way that a child could understand.\",\n",
    "crew     Answer=\"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD UP GRADIO TO TRY THE BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this code works\n",
    "\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "def generate_response(model: keras.Model, question: str, max_length: int = 250) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response for a single question using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): The fine-tuned language model.\n",
    "        question (str): The user's input question.\n",
    "        max_length (int): Maximum length of the generated answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer text.\n",
    "    \"\"\"\n",
    "    prompt_template = \"Question:\\n{question}\\n\\nAnswer:\\n\"\n",
    "    prompt = prompt_template.format(question=question)\n",
    "\n",
    "    try:\n",
    "        # Generate response using the model's generate method\n",
    "        generated_response = model.generate(prompt, max_length=max_length)\n",
    "\n",
    "        # Decode the generated response based on its type\n",
    "        if isinstance(generated_response, tf.Tensor):\n",
    "            generated_text = generated_response.numpy().decode('utf-8')\n",
    "        elif isinstance(generated_response, bytes):\n",
    "            generated_text = generated_response.decode('utf-8')\n",
    "        else:\n",
    "            generated_text = generated_response  # Assume it's already a string\n",
    "\n",
    "        # Extract the answer part by removing the prompt\n",
    "        answer_text = generated_text[len(prompt):].strip()\n",
    "        return answer_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"⚠️ Error generating answer: {e}\"\n",
    "\n",
    "# Define the chatbot function for Gradio\n",
    "def chatbot_gradio(user_input, model):\n",
    "    \"\"\"\n",
    "    Gradio-compatible chatbot function.\n",
    "    \n",
    "    Args:\n",
    "        user_input (str): The user's input question.\n",
    "        model (keras.Model): The fine-tuned language model.\n",
    "\n",
    "    Returns:\n",
    "        str: The chatbot's answer.\n",
    "    \"\"\"\n",
    "    if not user_input.strip():\n",
    "        return \"Chatbot: I'm here to help! Please enter a question.\"\n",
    "\n",
    "    answer = generate_response(model, user_input, max_length=250)\n",
    "    return answer\n",
    "\n",
    "# Example usage with Gradio:\n",
    "if gemma_lm:\n",
    "    # Create Gradio interface\n",
    "    interface = gr.Interface(\n",
    "        fn=lambda user_input: chatbot_gradio(user_input, gemma_lm),\n",
    "        inputs=\"text\",\n",
    "        outputs=\"text\",\n",
    "        title=\"Gemma 2b Language Model Chatbot\",\n",
    "        description=\"Enter a question, and the chatbot will respond.\"\n",
    "    )\n",
    "\n",
    "    # Launch the interface\n",
    "    interface.launch(share=True)\n",
    "else:\n",
    "    print(\"⚠️ Model not loaded. Skipping chatbot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CLEAN UP THE ENVIORNMENT FREE MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from numba import cuda\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get the current and peak GPU memory usage.\"\"\"\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if not gpus:\n",
    "        return 0, 0  # No GPU available\n",
    "    memory_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "    return memory_info['current'], memory_info['peak']\n",
    "\n",
    "# Get GPU memory usage before cleanup\n",
    "before_memory, before_peak = get_gpu_memory_usage()\n",
    "\n",
    "# Delete any variables holding models or large objects\n",
    "try:\n",
    "    del gemma_lm  # Delete the model if it exists\n",
    "except NameError:\n",
    "    pass  # If gemma_lm doesn't exist, continue\n",
    "\n",
    "# Clear TensorFlow GPU memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Run garbage collection to free memory\n",
    "gc.collect()\n",
    "\n",
    "# Reset GPU memory\n",
    "try:\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()\n",
    "except Exception as e:\n",
    "    print(f\"CUDA reset failed: {e}\")\n",
    "\n",
    "# Get GPU memory usage after cleanup\n",
    "after_memory, after_peak = get_gpu_memory_usage()\n",
    "\n",
    "# Calculate and display the memory cleared\n",
    "cleared_memory = before_memory - after_memory\n",
    "print(f\"Memory before cleanup: {before_memory / 1024**2:.2f} MB\")\n",
    "print(f\"Memory after cleanup: {after_memory / 1024**2:.2f} MB\")\n",
    "print(f\"Memory cleared: {cleared_memory / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5864618,
     "sourceId": 9611153,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3533,
     "modelInstanceId": 5171,
     "sourceId": 10260,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 897.635437,
   "end_time": "2024-02-21T09:52:28.59121",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T09:37:30.955773",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
